<FrameworkSwitchCourse {fw} />

# Clasificaci√≥n de Tokens[[token-classification]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={7}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section2_tf.ipynb"},
]} />

{/if}

La primera aplicaci√≥n que exploraremos en Clasificaci√≥n de Tokens. Esta tarea gen√©rica contempla cualquier problema que pueda ser formulado como "la atribuci√≥n de una etiqueta a cada token de una oraci√≥n" tales como:

- **Reconocimiento de Entidades nombradas (Named Entity Recognition (NER))**: Encontrar identidades (tales como personas, ubicaciones, u organizaciones) en una oraci√≥n. Esto se puede formular como la atribuci√≥n de una etiqueta a cada token teniendo una clase por identidad o una clase para "no entidad".
- **Etiquetado de Parte de Discurso (Part-of-speech tagging (POS))**: Marcar cada palabra en una oraci√≥n en correspondencia con una parte de discurso en particular (tales como sustantivos, verbos, adjetivos, etc.).
- **Fragmentaci√≥n (Chunking)**: Encontrar los tokens que pertenecen a la misma entidad. Esta tarea (la cual se puede combinar con NER o POS) se puede formular como la atribuci√≥n de una etiqueta (usualmente `B-`) a cualquiera de los tokens que est√©n al inicio de un fragmento, otra etiqueta (usualmente `I-`) a los tokens que est√°n dentro de un fragmento, y una tercera etiqueta (usualmente `O-`) a los tokens que no pertenecen a ning√∫n fragmento.

<Youtube id="wVHdVlPScxA"/>

Por supuesto, hay muchos otros tipos de problemas de clasificaci√≥n de tokens; esos son s√≥lo unos pocos ejemplos representativos. En esta secci√≥n, haremos fine-tuning de un modelo (BERT) en una tarea de NER, el cual luego ser√° capaz de calcular predicciones como esta:


<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

Puedes encontrar el modelo que entrenaremos y subiremos al Hub y revisar sus predicciones [ac√°](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn).


## Preparando los datos[[preparing-the-data]]

Primero lo primero, necesitamos un conjunto de datos adecuado para hacer clasificaci√≥n de tokens. En esta secci√≥n usaremos el [CoNLL-2003 dataset](https://huggingface.co/datasets/conll2003) el cual contiene art√≠culos de la cadena de noticias Reuters.

<Tip>

üí° Mientras tu conjunto de datos consista de textos divididos en palabras con sus etiquetas correspondientes, ser√°s capaz de adaptar los datos los procedimientos de procesamiento de datos descritros ac√° a tu propio conjunto de datos. Refi√©rete al [Cap√≠tulo 5](/course/chapter5) si necesitas recordar como cargar tus datos personalizados en un `Dataset`.

</Tip>

### El conjunto de datos CoNLL-2003[[the-conll-2003-dataset]]

Para cargar el conjunto de datos CoNLL-2003, usamos el m√©todo `load_dataset()` de la librer√≠a ü§ó Datasets:

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

Esto descargar√° y almacenar√° el conjunto de datos, como vimos en el [Cap√≠tulo 3](/course/chapter3) para el conjunto de datos GLUE MRPC. Inspeccionando este objeto, nos muestra las columnas presentes y la separaci√≥n entre los conjuntos de entrenamiento, validaci√≥n y prueba:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```
En particular, podemos ver que el conjunto de datos contiene etiquetas para las tres tareas que mencionamos anteriormente: NER, POS y fragmentaci√≥n. Una gran diferencia de otros conjuntos de datos es que los textos de entradas no est√°n presentados como oraciones o documentos, sino como listas de palabras (la √∫ltima columna se llama `tokens`, pero contiene palabras en el sentido de que √©stas con entradas pre-tokenizadas que a√∫n necesitan pasar por el tokenizador para la tokenizaci√≥n por subpalabras).

Miremos al primer elemento del conjunto de entrenamiento:

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

Dado que queremos realizar reconocimiento de entidades nombradas, miraremos a las etiquetas para NER:

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

Esas son las etiquetas como enteros listas para el entrenamiento, pero no son necesariamente √∫tiles cuando queremos inspeccionar los datos. Al igual que para la clasificaci√≥n de textos, podemos acceder a la correspondencia entre esos enteros y los nombres de etiquera mirando el atributo `features` de nuestro conjunto de datos:

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

Estas columnas contienen elementos que que son secuencias de `ClassLabel`s. El tipo de elementos de la secuencia es est√° en el atributo `feature` de `ner_feature`, y podemos acceder la lista de nombres mirando el atributo de `names` de `feature`:

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

Ya vimos que estas etiquetas cuando profundizamos en el pipeline de `clasificaci√≥n de tokens` en el [Cap√≠tulo 6](/course/chapter6/3), pero como un r√°pido recordatorio:

- `O` significa que la palabra no corresponde a ninguna entidad.
- `B-PER`/`I-PER` significa que la palabra corresponde al inicio de/est√° dentro de una entidad *persona*.
- `B-ORG`/`I-ORG` significa que la palabra corresponde al inicio de/est√° dentro de una entidad *organizaci√≥n*.
- `B-LOC`/`I-LOC` significa que la palabra corresponde al inicio de/est√° dentro de una entidad *ubicaci√≥n*.
- `B-MISC`/`I-MISC` significa que la palabra corresponde al inicio de/est√° dentro de una entidad *miscel√°neo*.

Ahora decodificando las etiquetas que vimos anteriormente nos da esto:

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

Y por ejemplo mezclando las etiquetas de `B-` y `I-`, esto es lo que el mismo c√≥digo nos da en el elemento de √≠ndice 4 del conjunto de entrenamiento:

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

Como podemos ver, las entidades que se extienden dos palabras, como "European Union" y "Werner Zwingmann", son atribuidos a la etiqueta `B-` para la primera palabra y la etiqueta `I-` para el segundo.

<Tip>

‚úèÔ∏è **Tu turno!** Imprime las mismas dos oraciones con sus etiquetas de POS o Fragmentaci√≥n.

</Tip>

### Processing the data[[processing-the-data]]

<Youtube id="iY2AZYdZAr0"/>

Como es usual, nuestros textos necesitan ser convertidos a IDs de token antes de que el modelo pueda hacer sentido de ellos. Como vimos en el [Chapter 6](/course/chapter6/), una gran diferencia en el caso de tareas de clasificaci√≥n de tokens es que tenemos entradas pre-tokenizadas. Afortunadamente, la API de tokenizadores puede lidiar con eso de manera sumamente f√°cil; s√≥lo necesitamos advertir al `tokenizador` con una alerta especial.

Para comenzar, creemos nuestro objeto `tokenizer`. Como dijimos antes, estaremos usando un modelo pre-entrenado BERT, por lo que empezaremos descargando y almacenando el tokenizador asociado:

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

Puedes reemplazar el `model_checkpoint` (punto de control) con cualquier otro modelo que prefieras desde el [Hub](https://huggingface.co/models), o con una carpeta local en la cual hayas guardado un modelo pre-entrenado y un tokenizador. La √∫nica restricci√≥n es que el tokenizador necesita estar respaldado por la librer√≠a ü§ó Tokenizers, para que haya un versi√≥n "r√°pida" disponible. Puedes ver todas las arquitecturas que vienen con una versi√≥n r√°pida en [esta tabla gigante](https://huggingface.co/transformers/#supported-frameworks), y para revisar que el objeto `tokenizer` est√° realmente respaldado por ü§ó Tokenizers puedes mirar su atributo `is_fast`:

```py
tokenizer.is_fast
```

```python out
True
```

Para tokenizar una entrada pre-tokenizada, podemos usar nuestro `tokenizer` como es usuar, y s√≥lo a√±adir `is_split_into_words=True`:

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

Como podemos ver, el tokenizador a√±adi√≥ los tokens especiales utilizados por el modelo (`[CLS]` al inicio y `[SEP]` al final) y dej√≥ la mayor√≠a de las palabras tal sin modificar. La palabra `lamb`, sin embargo, fue tokenizada en dos subpalabras, `la` y `##mb`. Esto introduce una inconsistencia entre nuestras entradas (inputs) y las etiquetas: la lista de etiquetas tiene s√≥lo 9 elementos, mientras que nuestro input ahora tiene 12 tokens. Tomar en cuentra los tokens especiales es f√°cil (sabemos que est√°n al inicio y al final), pero tambi√©n necesitamos asegurarnos que alineamos todas las etiquetas con las palabras correspondientes.

Afortunadamente, dado que estamos utilizando un tokenizador r√°pido tenemos acceso a los superpoderes de ü§ó Tokenizers, lo que significa que podemos mapear f√°cilmente cada token con su palabra correspondiente (como se vi√≥ en el [Cap√≠tulo 6](/course/chapter6/3)):

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

Con muy poquito trabajo, podemos expandir nuestra lista de etiquetas para emparejarlo con los tokens. La primera regla que aplicaremos es que los tokens especiales obtienen una etiqueta de `-100`. Esto es porque por defecto `-100` es un √≠ndice que es ignorado por la funci√≥n de p√©rdida que utilizaremos (entrop√≠a cruzada (cross-entropy)). Luego, cada token obtiene la misma etiqueta a medida que el otken que empez√≥ la palabra est√© dentro, dado que son parte de la misma entidad. Para tokens dentro de una palabra pero que no est√°n al inicio reemplazamos `B-` con `I-` (dado que el token no comienza la entidad):

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

Intent√©moslo en nuestra primera oraci√≥n:

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

Como podemos ver, nuestra funci√≥n a√±adi√≥ el `-100` para los dos tokens especiales al inicio y al final, y un nuevo `0` para nuestra palabra que fue separada en dos tokens.

<Tip>

‚úèÔ∏è **Tu turno!** algunos investigadores prefieren atribuir s√≥lo una etiqueta por palabra, y asignar `-100` a los otros subtokens en la palabra dada. Esto es para evitar palabrar largas palabras que se separan en montones de subtokens que contribuyen significativamente a la p√©rdida. Cambia la funci√≥n previa para alinear la etiquetas con los Input IDs siguiendo esta regla.

</Tip>

Para preprocesar nuestro conjunto de datos completo, necesitamos tokenizar todos los inputs y aplicar `align_labels_with_tokens()` sobre todas las etiquetas. Para aprovechar la velocidad de nuestro tokenizador r√°pido, es mejor tokenizar montones de textos al mismo tiempo, por lo que escribiremos una funci√≥n que procese listas de ejemplos y el m√©todo `Dataset.map()` con la opci√≥n `batched=True`. La √∫nica diferencia con nuestro ejemplo previo es que la funci√≥n `word_ids()` necesita obtener el √≠ndice del ID de palabra que queremos cuando las entradas del tokenizador son listas de textos (o en nuestro caso, listas de listas de palabras), por lo que agregamos eso tambi√©n:

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

Nota que no hemos rellenado nuestras entradas a√∫n, haremos eso m√°s tarde,cuando creemos los lotes con un `data collator`.

Ahora podemos aplicar todo ese preprocesamiento de una vez en las otras separaciones de nuestro conjunto de datos:

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

¬°Hemos hecho la parte m√°s dif√≠cil! Ahora que los datos se han preprocesado, el entrenamiento se ver√° mucho m√°s parecido a lo que hicimos en el [Cap√≠tulo 3](/course/chapter3).

{#if fw === 'pt'}

## Ajustar el modelo con la API de `Trainer`[[fine-tuning-the-model-with-the-trainer-api]]

El c√≥digo usando `Trainer` ser√° el mismo de antes; los √∫nicos cambios son la manera en que los datos son agrupados en un lote y la funci√≥n de c√°lculo de la m√©trica.

{:else}

## Ajustar el modelo con Keras[[fine-tuning-the-model-with-keras]]

El c√≥digo usando Keras ser√° muy similar al anterior; los √∫nicos cambios son la manera en que los datos son agrupados en un lote y la funci√≥n de c√°lculo de la m√©trica.

{/if}


### Agrupaci√≥n de los Datos[[data-collation]]

No podemos usar s√≥lo un `DataCollatorWithPadding` como en el [Cap√≠tulo 3](/course/chapter3) porque eso s√≥lo rellena las entradas (input IDs, m√°scara de atenci√≥n (attention mask), y los IDs de tipo de token). Ac√° nuestras etiquetas deber√≠an rellenarse de la misma manera que las entradas para que pudieran mantener el mismo tama√±o, usando `-100` para que las predicciones correspondientes sean ignoradas en el c√°lculo de la p√©rdida.

Esto se hace por medio de un [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). Al igual que el `DataCollatorWithPadding`, √©ste toma el `tokenizer` usado para procesar las entradas:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

Para probar esto en unas pocas muestras, podemos llamarlo en una lista de ejemplos de nuestro conjunto de entrenamiento tokenizdo:

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

Comparemos esto con las etiquetas apra el primer y segundo elemento de nuestro conjunto de datos:

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

Como podemos ver, el segundo conjunto de etiquetas ha sido rellenado al largo del primero usando `-100`.

{:else}

Nuestro data collator est√° listo!! Ahora usemoslo para hacer un `tf.data.Dataset` con el m√©todo `to_tf_dataset()`. Tambi√©n puedes usar `model.prepare_tf_dataset()` para hacer esto con un poco menos de c√≥digo repetitivo - ver√°s esto en alguna de las otras secciones de este cap√≠tulo.

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

Siguiente parada: el Modelo propiamente tal.

{/if}

{#if fw === 'tf'}

### Definiendo el modelo[[defining-the-model]]

Dado que estamos trabajando en problema de clasificaci√≥n de tokens, usaremos la clase `TFAutoModelForTokenClassification`. Lo principal es que al definir este modelo pasemos informaci√≥n acerca del n√∫mero de etiquetas que tenemos. La manera m√°s f√°cil de hacer esto es pasar el n√∫mero con el argumento `num_labels`, pero si queremos un widget de inferencia que funcione como el que vimos en el inicio de esta secci√≥n, es mejor ajustar las correspondencias de etiquetas correctas.

Ellas deber√≠an ajustarse usando dos diccionarios, `id2label` y `label2id`, que contengan el mapeo desde el ID a la etiqueta y vice versa:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Ahora podemos pasarlos al m√©todo `TFAutoModelForTokenClassification.from_pretrained()` y se ajustar√° la configuraci√≥n del modelo, para luego ser guardado y subido de manera apropiada al Hub:

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Al igual que cuando definimos nuestro `TFAutoModelForSequenceClassification` en el [Cap√≠tulo 3](/course/chapter3), creando el modelo se emite una advertencia de que algunos pesos no se usaron (los pesos de la cabeza de pre-entrenamiento) y otros pesis son inicializados de manera aleatoria (los de la nueva cabeza de clasificaci√≥n de tokens), y que este modelo deber√≠a ser entrenado. Haremos eso en un minuto, pero primero revisemos que nuestro modelo tiene el n√∫mero correcto de etiquetas:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö†Ô∏è Si tienes un modelo con un n√∫mero incorrecto de etiquetas, obtendr√°s un error oscuro al momento de llamar `model.fit()` m√°s tarde. Esto puede ser molesto de depurar, por lo que aseg√∫rate de hacer esta revisi√≥n para confirmar que tienes el n√∫mero esperado de etiquetas.

</Tip>

### Ajuste del modelo (Fine-tuning del Modelo)[[fine-tuning-the-model]]

Estamos listos para entrenar nuesto modelo! Primero tenemos que hacer un poco de orden: deber√≠amos registrarnos en Hugging Face y definir nuestros hiperpar√°metros de entrenamiento. Si est√°s trabajando en un notebook, hay una funci√≥n para ayudarte con esto:

```python
from huggingface_hub import notebook_login

notebook_login()
```

Esto mostrar√° un widget donde puedes ingresar tus credenciales de Hugging Face.

Si es que no est√°s trabajando en un notebook, s√≥lo escribe la siguiente l√≠nea en tu terminal:

```bash
huggingface-cli login
```

Luego de registrarte, podemos preparar todo lo que necesitamos para compilar nuestro modelo. ü§ó Transformers provee una funci√≥n llamada `create_optimizer()` que te dar√° un optimizador `AdamW` con una configuraci√≥n con los ajustes apropiados para el decremento de pesos (weight decay) y el decremento de la tasa de aprendizaje (Learning Rate Decay), los cuales mejorar√°n el rendimiento del modelo comparado con el optimizador `Adam` por defecto: 

```python
from transformers import create_optimizer
import tensorflow as tf

# Train in mixed-precision float16
# Comment this line out if you're using a GPU that will not benefit from this
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

Notar tambi√©n que no suministramos un argumento `loss` a `compile()`. Esto es porque los modelos pueden calcular la p√©rdida internamenta -- si tu compilas sin una p√©rdida y suministras tus etiquetas en el diccionario de entrada (como lo hacemos en nuestros conjuntos de datos), luego el modelo se entranar√° usando esas p√©rdidas internas, las cuales ser√°n apropiadas para la tarea y el tipo de modelo que escogiste.

A continuaci√≥n, definimos `PushToHubCallback` para subir nuestro modelo al Hub durante el entrenamiento, y ajustar el modelo con ese callback:

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```
Puedes especificar el nombre completo del repositorio al que quieras subirlo con el argumento `hub_model_id` (en particular, tendr√°s que usar este argumento en caso de que quieras subirlo a una organizaci√≥n). Por ejemplo, cuando subimos el modelo a la [organizaci√≥n del `huggingface-course`](https://huggingface.co/huggingface-course), agregamos `hub_model_id="huggingface-course/bert-finetuned-ner"`. Por defecto, el repositorio usado ser√° parte de tu namespace y se nombrar√° como el directorio de salida utilizado, por lo que en nuestro caso ser√° `"cool_huggingface_user/bert-finetuned-ner"`.

<Tip>
üí° Si el directorio de salida que est√°s usando ya existe, necesita ser una copia local del repositorio al que quieres subir. Si no lo es, obtendr√°s un error al llamar `model.fit()` y tendr√°s que utilizar un nuevo nombre.

</Tip>

Nota que mientras ocurre el entrenamiento, cada vez que el modelo es guardado (en este caso, cada √©poca) se sube al Hub en segundo plano. De esta manera, ser√°s capaz de reiniciar tu entrenamiento en otra m√°quina si es necesario. 

En esta etapa, puedes usar un widget de inferencia en el Model Hub para probar tu modelo y compartirlo con tus amigos. Has ajustado tu modelo (fine-tuned) un modelo en una tarea de clasificaci√≥n de tokens de manera exitosa -- felicitaciones! Pero, qu√© tan bueno es nuestro modelo en realidad? Deber√≠amos evaluar algunas m√©tricas para averiguarlo. 

{/if}


### Metricas[[metrics]]

{#if fw === 'pt'}

Para hacer que el `Trainer` calcule la m√©trica para cada √©poca, necesitaremos definir una funci√≥n `compute_metrics()` que tome los arreglos de predicciones y las etiquetas, y retorne un diccionario con el nombre de las m√©tricas y sus valores. 

El framework tradicionalmente utilizado para evaluar la predicci√≥n de clasficaci√≥n de tokens es [*seqeval*](https://github.com/chakki-works/seqeval). Para usar esta m√©trica, primero necesitamos instalar la librer√≠a *seqeval*:

```py
!pip install seqeval
```

Luego podemos cargarla utilizando la funci√≥n `evaluate.load()` como lo hicimos en el [Cap√≠tulo 3](/course/chapter3):

{:else}

El framework tradicionalmente utilizado para evaluar la predicci√≥n de clasficaci√≥n de tokens es [*seqeval*](https://github.com/chakki-works/seqeval). Para usar esta m√©trica, primero necesitamos instalar la librer√≠a *seqeval*:

```py
!pip install seqeval
```

Luego podemos cargarla utilizando la funci√≥n `evaluate.load()` como lo hicimos en el [Chapter 3](/course/chapter3):

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

Esta m√©trica no se comporta como la exactitud standard (standard accuracy): en realidad tomar√° una lista de etiquetas como strings, no enteros, por lo que necesitar√° decodificar las predicciones y las etiquetas totalmente antes de pasarlas a la m√©trica. Veamos como funciona. Primero, obtendremos nuestras etiquetas para nuestro primer ejemplo de entrenamiento: 

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

Luego podemos crear predicciones falsas cambiando el valor del √≠ndice 2:

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

Nota que la m√©trica tome una lista de predicciones (no s√≥lo una) y una lista de etiquetas. Esta es la salida:

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

¬°Esto nos devuelve un mont√≥n de informaci√≥n! Obtenemos la precisi√≥n, recal y el F1 Score para cada entidad separada, como tambi√©n un valor general. Para el c√°lculo de nuestra m√©trica s√≥lo queremos mantener el puntaje general, pero si√©ntete libre de modificar la funci√≥n `compute_metrics()` para retornar todas las m√©tricas que te gustar√≠a reportar. 

Esta funci√≥n `compute_metrics()` primero toma el argmax de los logits para convertirlos a predicciones (como es usuarl, los logits y las probabilidades est√°n en el mismo orden, por lo tanto no necesitamos aplicar la funci√≥n softmax). Luego tenemos que convertir etiquetas y predicciones de enteros a strings. Removemos todos los valores donde la etiqueta es `-100`, luego pasamos los resultados al m√©todo `metric.compute()`:

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

Ahora que esto est√° listo, estamos casi listos para definir nuestro `Trainer`. ¬°S√≥lo necesitamos un modelo para ajustar (fine-tune)!

{:else}

¬°Esto nos devuelve un mont√≥n de informaci√≥n! Obtenemos la precisi√≥n, recal y el F1 Score para cada entidad separada, como tambi√©n un valor general. Ahora veamos qu√© ocurre si es que tratamos de usar las predicciones de nuestro modelo para calcular los puntajes reales.

A Tensorflow no le gusta concatenar nuestras predicciones, porque tienen un largo de secuencia variable. Esto significa que no podemos usar `model.predict()` directamente -- pero eso no nos va a detener. Obtendremos algunas predicciones un lote a la vez y las concatenaremos en una larga lista, eliminando los tokens `-100` que indicar enmascaramiento/relleno, para luego calcular las m√©tricas en la lista:


```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

¬øC√≥mo le fue a tu modelo, comparado con el nuestro? ¬°Si obtuviste n√∫meros similares, tu entrenamiento fue un √©xito!

{/if}

{#if fw === 'pt'}

### Definiendo el modelo[[defining-the-model]]

Dado que estamos trabajando con un problema de clasificaci√≥n de tokens, usaremos la clase `AutoModelForTokenClassification`. El principal aspecto a recordar al momento de definir este modelo es pasar informaci√≥n acerca del n√∫mero de etiquetas que tenemos. La manera m√°s simple de hacer esto es pasar dicho n√∫mero utilizando el argumento `num_labels`, pero si queremos tener un buen widget de inferencia que funcione como el que vimos al inicio de esta secci√≥n, es mejor definir las correspondencias con las etiquetas. 

Esto deber√≠a definirse mediante dos diccionarios, `id2label` y `label2id`, el cual contiene el mapeo de ID a etiqueta y viceversa:

```py
id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

Ahora basta con pasarlos al m√©todo `AutoModelForTokenClassification.from_pretrained()`, para ser definidos en la configuraci√≥n del modelo y ser guardados y subidos de manera apropiada al Hub:  

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

De la misma maneara como definimos nuestro `AutoModelForSequenceClassification` en el [Cap√≠tulo 3](/course/chapter3), crear el modelo env√≠a una advertencia que algunos pesos no no fueron usados (los de la cabeza del pre-entrenamiento) y que otros est√°n siendo inicializados de manera aleatoria (los de la nueva cabeza de clasificaci√≥n), y que este modelo deber√≠a entrenar. Haremos eso en un minuto, pero primero revisemos nuevamente que nuestro modelo tenga el n√∫mero correcto de etiquetas:

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

‚ö† Si tienes un modelo con el n√∫mero de etiquetas incorrecto, obtendr√°s un error bastante confuso al llamar al m√©todo `Trainer.train()`. (Algo como "CUDA error: device-side assert triggered"). Esta es la causa n√∫mero 1 de bugs reportados por los usuarios, por lo que aseg√∫rate de hacer este chequeo para confirmar que tienes el n√∫mero de etiquetas esperado. 

</Tip>

### Ajustando (Fine-tuning) el modelo[[fine-tuning-the-model]]

¬°Ya estamos listos para entrenar nuestro modelo! S√≥lo necesitamos hacer dos cosas antes de definir nuestro `Trainer`: registrarnos a Hugging Face y definir nuestros argumentos de entrenamiento. Si est√°s trabajando en un notebook, hay una funci√≥n para ayudarte con esto: 

```python
from huggingface_hub import notebook_login

notebook_login()
```

Esto mostrar√° un widget donde puedes ingresar tus credenciales de acceso a Hugging Face.

Si no est√°s trabajando en una notebook, s√≥lo escribe la siguiente l√≠nea en tu terminal:

```bash
huggingface-cli login
```

Una vez que esto est√° hecho, podemos definir nuestros `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

Ya has visto la mayor√≠a de esto antes: fijamos algunos hiperpar√°metros (como la tasa de aprendizaje, el n√∫mero de √©pocas por el cual entrenar, y el decremento de pesos), y especificamos `push_to_hub=True` para indicar que queremos guardar el modelo y evaluar lo al final de cada √©poca, y que queremos subir los resultados al Model Hub. Nota que puedes especificar el nombre del repositorio al que quieres subirlo con el argumento `hub_model_id` (en particular, tendr√°s que usar este argumento para subirlo a una organizaci√≥n). Por ejemplo, cuando subimos el modelo a la [organizaci√≥n del `huggingface-course`](https://huggingface.co/huggingface-course) agregamos `hub_model_id="huggingface-course/bert-finetuned-ner"` a `TrainingArguments`. Por defecto, el repositorio usado ser√° parte de tu namespace y se nombrar√° como el directorio de salida utilizado, por lo que en nuestro caso ser√° `"sgugger/bert-finetuned-ner"`.

<Tip>
üí° Si el directorio de salida que est√°s usando ya existe, necesita ser un clon local del repositorio al que quieres subir. En caso de no serlo, obtendr√°s un error al momento de definir tu `Trainer` y tendr√°s que definir un nuevo nombre.

</Tip>

Finalmente, s√≥lo pasamos todo al `Trainer` e iniciamos el entrenamiento:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

Nota que mientras ocurre el entrenamiento, cada vez que el modelo se guarda (en este caso, cada √©poca) es subido al Hub en segundo plano. De esta manera podr√°s reiniciar tu entrenamiento en otra m√°quina si es necesario. 

Una vez que se completa el entrenamiento, usamos el m√©todo `push_to_hub()` para asegurar que subamos la versi√≥n m√°s reciente del modelo:

```py
trainer.push_to_hub(commit_message="Training complete")
```

Este comando retorna la URL del commit que hicimos, en caso que quieras inspeccionarla:

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

El `Trainer` tambi√©n genera el bosquejo de una carta de modelo con todos los resultados de la evaluaci√≥n y la sube. En esta etapa, puedes utilizar un widget de inferencia en el Model Hub para probar tu modelo y compartilo con tus amigos. ¬°Has ajustado (fine-tuned) un modelo en una tarea de clasificaci√≥n de tokens -- felicitaciones!

Si quieres profundizar de manera un poco m√°s profunda en el ciclo de entrenamiento (training loop), ahora mostraremos c√≥mo hacer lo mismo utilizando ü§ó Accelerate.

## Ciclo de Entrenamiento a personalizado[[a-custom-training-loop]]

Ahora echemos una mirada al ciclo de entrenamiento completo, para que puedas personalizar las partes que necesites. Se parecer√° mucho a lo que hicimos en el [Cap√≠tulo 3](/course/chapter3/4), con algunos cambios para la evaluaci√≥n. 

### Preparando todo para el entrenamiento[[preparing-everything-for-training]]

Primero necesitamos construir los `DataLoader`s desde nuestros conjuntos de datos. Reusaremos nuestro `data_collator` como una `collate_fn` y mezclaremos el conjunto de entrenamiento, pero no el conjunto de validaci√≥n:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

A continuaci√≥n reinstanciamos nuestro modelo, para asegurarnos que no estamos continuando el ajuste (fine-tuning) de antes, sino que comenzando de un modelo BERT pre-entrenado nuevamente:

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

Luego necesitaremos un optimizador. Usaremos el cl√°sico `AdamW`, el cual es como un `Adam`, pero con una correccion en c√≥mo el decremento de pesos es aplicado: 

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Una vez que tenemos todos estos objetos, podemos enviarlos al m√©todo `accelerator.prepare()`:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>
üö® Si est√°s entrenando en una TPU, necesitar√°s mover todo el c√≥digo comenzando desde la celda anterior en una funci√≥n dedicada de entrenamiento. Mira el [Cap√≠tulo 3](/course/chapter3) para m√°s detalles. 

</Tip>

Ahora que hemos enviado nuestro `train_dataloader` al `accelerator.prepare()`, podemos usar su largo para calcular el n√∫mero de pasos de entrenamiento. Recuerda que debemos siempre hacer esto luego de preparar el dataloader, ya que el m√©todo cambiar√° su largo. Usamos un cl√°sico programa lineal variando el learning rate hasta 0:

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

Finalmente, para subir nuestro modelo al Hub, necesitamos crear un objeto `Repository` in una carpeta. Primero nos conectamos a Hugging Face, si es que no est√°s conectado a√∫n. Determinaremos el nombre del repositorio desde el ID del modelo que queremos darle a nuestro model (si√©ntete libre de reemplazar el `repo_name` con el nombre que t√∫ quieras; s√≥lo necesita contener tu nombre de usuario, que es lo que la funci√≥n `get_full_repo_name() hace): 

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

Luego podemos clonar el ese repositorio en una carpeta local. Si ya existe, esta carpeta local deber√≠a ser un clon existente del repositorio con el que est√°s trabajando:

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

Ahora podemos subir todo lo que guardemos en `output_dir` llamando el m√©todo `repo.push_to_hub()`. Esto nos ayudar√° a subir los modelos intermedios al final de cada √©poca. 

### Ciclo de Entrenamiento[[training-loop]]

Ahora estamos listos para escribir el ciclo de entrenamiento completo. Para simplificar la parte de evaluaci√≥n, definimos esta funci√≥n `postprocess()` que toma las predicciones y etiquetas y las convierte a listas de strings, como nuestro objeto `metric` espera:

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

Luego podemos escribir el ciclo de entrenamiento. Despu√©s de definir una barra de progreso para seguir c√≥mo va el entrenamiento, el ciclo tiene tres partes: 

- El entrenamiento propiamente tal, el cu√°l es la cl√°sica iteraci√≥n sobre el `train_dataloader`, pasada hacia adelante a trav√©s del modelo, luego pasada hacia atr√°s y el paso del optimizador. 
- La evaluaci√≥n, en la cual hay una novedad luego de obtener las salidas de nuestro modelo en un lote: dado que dos procesos han rellenado las entradas y las etiquetas a formas diferentes, necesitamos usar el `accelerator.pad_across_processes()` para hacer que las predicciones y las etiquetas tengan la misma forma que ten√≠an antes de llamar al m√©todo `gather()`. Si no hacemos esto, la evaluaci√≥n fallar√° o quedar√° colgada por siempre. Luego enviamos los resultados a `metric.add_batch()` y llamamos a `metric.compute()` una vez que el ciclo de evaluaci√≥n se acaba. 
- Guardar y subir el modelo, donde primero guardamos el modelo y el tokenizador, luego llamamos `repo.push_to_hub()`. Nota que usamos el argumento `blocking=False` para decirle al ü§ó Hub que suba en un proceso as√≠ncrono. De esta manera, el entrenamiento contin√∫a de manera normal y esta (larga) instrucci√≥n se ejecuta en segundo plano. 

Ac√° est√° el c√≥digo completo para el ciclo de entrenamiento: 

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

En caso que este sea la primera vez que ves un modelo guarddo con ü§ó Accelerate, tomemos un momento para inspeccionar las 3 l√≠neas de c√≥digo que le permiten funcionar: 

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

La primera l√≠nea es auto-explicativa: le dice a todos los procesos que esperen hasta que todos est√©n en esa etapa antes de continuar. Esto es para asegurar que tenemos el mismo model en cada proceso antes de guardar. Luego tomemos el `unwrapped_model`, el cual es el modelo base que definimos. El m√©todo `accelerator.prepare()` cambia el modelo para trabajar en entrenamiento distribuido, por lo que ya no tendr√° el m√©todo `save_pretrained()`; el `accelerator.unwrap_model()` deshace ese paso. Finalmente, llamamos a `save_pretrained()` pero haciendo que dicho m√©todo use `accelerator.save()` en vez de `torch.save()`.

Una vez que esto est√© listo, deber√≠as tener un modelo que produce resultados bastante similares a los que se obtuvieron con el `Trainer`. Puedes chequear el modelo que entrenamos usando este c√≥digo en [*huggingface-course/bert-finetuned-ner-accelerate*](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). Y si quieres probar cualquier modificaci√≥n al ciclo de entrenamiento, puedes implementar dichos cambios editando el c√≥digo mostrado arriba. 

{/if}

## Usando el modelo ajustado (fine-tuned)[[using-the-fine-tuned-model]]

Ya te hemos mostrado c√≥mo puedes usar el modelo ajustado (fine-tuned model) en el Hub con el widget de inferencia. Para usarlo localmente en un `pipeline`, s√≥lo tienes que especificar el identificador propio del modelo: 

```py
from transformers import pipeline

# Replace this with your own checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

¬°Genial! ¬°Nuestro modelo est√° funcionando tan bien c√≥mo el el modelo por defecto para este pipeline!
Great! Our model is working as well as the default one for this pipeline!
