<FrameworkSwitchCourse {fw} />

# Introducci√≥n[[introduction]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

En el [Cap√≠tulo 3](/course/chapter3), viste como hacer fine-tunning a un modelo de clasificaci√≥n de texto. En este cap√≠tulo, abordaremos las siguientes tareas de PLN m√°s comunes:

- Clasificaci√≥n de Tokens
- Modelamiento de Lenguaje Enmascarado (`Masked language modeling`) (como en BERT)
- Resumen (`Summarization`)
- Traducci√≥n (`Translation`)
- Pre-entrenamiento de modelamiento de causalidad de Lenguage (`Causal language modeling pretraining`) (como GPT-2)
- Preguntas y Respuestas (Question answering)

{#if fw === 'pt'}

Para hacer esto, necesitar√°s hacer uso de todo lo que que has aprendido acerca de la API de `Trainer` y la librer√≠a ü§ó Accelerate en el [Cap√≠tulo 3](/course/chapter3), la librer√≠a ü§ó Datasets en el [Cap√≠tulo 5](/course/chapter5), y la librer√≠a ü§ó Tokenizers en el [Cap√≠tulo 6](/course/chapter6). Tambi√©n subiremos nuestros resultados al Model Hub, como lo hicimos en el [Cap√≠tulo 4](/course/chapter4), por lo que este es el cap√≠tulo donde unimos todo!

Cada secci√≥n puede ser le√≠da de manera independiente y mostraremos como entrenar un modelo con la API de `Trainer` o con tu propio bucle de entrenamiento, usando ü§ó Accelerate. Si√©ntete libre de saltarte cualquier parte y enfocarte en los que m√°s te interesen: la API de `Trainer` es genial para hacer fine-tuning o entrenar tu modelo sin preocuparte acerca de lo que pasa por detr√°s, mientras que el bucle de entrenamiento con `Accelerate` te permitir√° personalizar cualquier parte quieras m√°s f√°cilmente.

{:else}

Para hacer esto, necesitar√°s hacer uso de todo lo que has aprendido acerca del entrenamiento de modelos con la API de Keras en el [Cap√≠tulo 3](/course/chapter3), la librer√≠a ü§ó Datasets en el [Cap√≠tulo 5](/course/chapter5), y la librer√≠a ü§ó Tokenizers en el [Cap√≠tulo 6](/course/chapter6). Tambi√©n subiremos nuestros resultados al Model Hub, como lo hicimos en el [Cap√≠tulo 4](/course/chapter4), por lo que este es el cap√≠tulo donde unimos todo!

Cada secci√≥n se puede leer de manera independiente.

{/if}


<Tip>

Si lees las secciones de manera consecutiva, notar√°s que tienen algo de c√≥digo y prosa en com√∫n. La repetici√≥n es intencional, para permitir sumergirte (o volver m√°s tarde) en cualquier tarea que te interese y encuentras un ejemplo funcional completo.

</Tip>
